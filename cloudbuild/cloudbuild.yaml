steps:
# 0. Create a Docker image containing hadoop-connectors repo
- name: 'gcr.io/cloud-builders/docker'
  id: 'docker-build'
  args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit', '-f', 'cloudbuild/Dockerfile', '.']

# 1. Build the connector and download dependencies without running tests.
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'check'
  waitFor: ['docker-build']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'check']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 2. Build the connector and download dependencies without running tests.
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'build'
  waitFor: ['check']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'build']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 3. Run unit tests for Hive 1
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'unit-tests-hive1'
  waitFor: ['build']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'unittest_hive1']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 4. Run unit tests for Hive 2
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'unit-tests-hive2'
  waitFor: ['build']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'unittest_hive2']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 5. Run unit tests for Hive 3
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'unit-tests-hive3'
  waitFor: ['build']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'unittest_hive3']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 6. Run integration tests for Hive 1
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'integration-tests-hive1'
  waitFor: ['unit-tests-hive1']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest_hive1']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'
    - 'BIGQUERY_KMS_KEY_NAME=${_BIGQUERY_KMS_KEY_NAME}'

# 7. Run integration tests for Hive 2
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'integration-tests-hive2'
  waitFor: ['unit-tests-hive2']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest_hive2']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'
    - 'BIGQUERY_KMS_KEY_NAME=${_BIGQUERY_KMS_KEY_NAME}'

# 8. Run integration tests for Hive 3
- name: 'gcr.io/$PROJECT_ID/dataproc-hive-bigquery-connector-presubmit'
  id: 'integration-tests-hive3'
  waitFor: ['unit-tests-hive3']
  entrypoint: 'bash'
  args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest_hive3']
  env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'
    - 'BIGQUERY_KMS_KEY_NAME=${_BIGQUERY_KMS_KEY_NAME}'

# Tests should take under 120 mins
timeout: 7200s

options:
  machineType: 'N1_HIGHCPU_32'
